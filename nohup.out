[2025-02-20 20:52:25,022] [[32m    INFO[0m]: PyTorch version 2.5.1 available. (config.py:54)[0m
INFO 02-20 20:52:30 __init__.py:183] Automatically detected platform cuda.
[2025-02-20 20:52:30,579] [[32m    INFO[0m]: --- LOADING MODEL --- (pipeline.py:178)[0m
[2025-02-20 20:52:31,123] [[32m    INFO[0m]: --- LOADING TASKS --- (pipeline.py:205)[0m
[2025-02-20 20:52:31,191] [[33m WARNING[0m]: If you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`. (registry.py:136)[0m
[2025-02-20 20:52:31,191] [[32m    INFO[0m]: Found 4 custom tasks in /home/jiaminghui/Public/huggingface/modules/datasets_modules/datasets/evaluate/b2a4ce76e8d85795ef936e56c54ed53670073f8a39f76f8cb8cc500a4affbd5a/evaluate.py (registry.py:141)[0m
[2025-02-20 20:52:31,194] [[32m    INFO[0m]: Idavidrein/gpqa gpqa_diamond (lighteval_task.py:187)[0m
[2025-02-20 20:52:31,194] [[33m WARNING[0m]: Careful, the task custom|gpqa:diamond is using evaluation data to build the few shot examples. (lighteval_task.py:261)[0m
Using the latest cached version of the dataset since Idavidrein/gpqa couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-02-20 20:52:31,195] [[33m WARNING[0m]: Using the latest cached version of the dataset since Idavidrein/gpqa couldn't be found on the Hugging Face Hub (offline mode is enabled). (load.py:1444)[0m
Found the latest cached dataset configuration 'gpqa_diamond' at /home/jiaminghui/Public/huggingface/datasets/Idavidrein___gpqa/gpqa_diamond/0.0.0/90b8e5be2b1d3d2dbfe016cdab47981150600c4a (last modified on Thu Dec  5 16:24:15 2024).
[2025-02-20 20:52:31,196] [[33m WARNING[0m]: Found the latest cached dataset configuration 'gpqa_diamond' at /home/jiaminghui/Public/huggingface/datasets/Idavidrein___gpqa/gpqa_diamond/0.0.0/90b8e5be2b1d3d2dbfe016cdab47981150600c4a (last modified on Thu Dec  5 16:24:15 2024). (cache.py:94)[0m
[2025-02-20 20:52:31,483] [[32m    INFO[0m]: --- INIT SEEDS --- (pipeline.py:234)[0m
[2025-02-20 20:52:31,483] [[32m    INFO[0m]: --- RUNNING MODEL --- (pipeline.py:439)[0m
[2025-02-20 20:52:31,483] [[32m    INFO[0m]: Running RequestType.GREEDY_UNTIL requests (pipeline.py:443)[0m
[2025-02-20 20:52:31,609] [[33m WARNING[0m]: You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring. (data.py:260)[0m
Splits:   0%|          | 0/1 [00:00<?, ?it/s][2025-02-20 20:52:31,720] [[33m WARNING[0m]: context_size + max_new_tokens=35607 which is greather than self.max_length=32768. Truncating context to 0 tokens. (vllm_model.py:268)[0m
[2025-02-20 20:52:34,650] [[32m    INFO[0m]: Started a local Ray instance. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m (worker.py:1832)[0m
[36m(run_inference_one_model pid=2518531)[0m Calling ray.init() again after it has already been called.
[36m(run_inference_one_model pid=2518531)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[36m(run_inference_one_model pid=2518532)[0m Calling ray.init() again after it has already been called.
[36m(run_inference_one_model pid=2518531)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.09it/s]
[36m(run_inference_one_model pid=2518531)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.09it/s]
[36m(run_inference_one_model pid=2518531)[0m 
[36m(run_inference_one_model pid=2518532)[0m 
[36m(run_inference_one_model pid=2518531)[0m Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
[36m(run_inference_one_model pid=2518532)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[36m(run_inference_one_model pid=2518532)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s][32m [repeated 2x across cluster][0m
[36m(pid=2518532)[0m INFO 02-20 20:52:40 __init__.py:183] Automatically detected platform cuda.
[36m(run_inference_one_model pid=2518532)[0m WARNING 02-20 20:52:40 config.py:2492] User-specified max_model_len (32768) is greater than the derived max_model_len (max_position_embeddings=4096 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.
[36m(run_inference_one_model pid=2518532)[0m INFO 02-20 20:52:48 config.py:526] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward', 'score'}. Defaulting to 'generate'.
[36m(run_inference_one_model pid=2518532)[0m INFO 02-20 20:52:48 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='results/Qwen-2.5-1.5B-Simple-RL', speculative_config=None, tokenizer='results/Qwen-2.5-1.5B-Simple-RL', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config=None, tokenizer_revision=main, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=results/Qwen-2.5-1.5B-Simple-RL, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
[36m(pid=2518531)[0m INFO 02-20 20:52:40 __init__.py:183] Automatically detected platform cuda.
[36m(run_inference_one_model pid=2518531)[0m WARNING 02-20 20:52:41 config.py:2492] User-specified max_model_len (32768) is greater than the derived max_model_len (max_position_embeddings=4096 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:52:48 config.py:526] This model supports multiple tasks: {'generate', 'score', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
[36m(run_inference_one_model pid=2518532)[0m INFO 02-20 20:52:49 ray_distributed_executor.py:153] use_ray_spmd_worker: False
[36m(pid=2518533)[0m INFO 02-20 20:52:54 __init__.py:183] Automatically detected platform cuda.
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:52:48 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='results/Qwen-2.5-1.5B-Simple-RL', speculative_config=None, tokenizer='results/Qwen-2.5-1.5B-Simple-RL', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config=None, tokenizer_revision=main, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=results/Qwen-2.5-1.5B-Simple-RL, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:52:49 ray_distributed_executor.py:153] use_ray_spmd_worker: False
[36m(RayWorkerWrapper pid=2518533)[0m INFO 02-20 20:52:55 cuda.py:235] Using Flash Attention backend.
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:52:59 utils.py:938] Found nccl from library libnccl.so.2
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:52:59 pynccl.py:67] vLLM is using nccl==2.21.5
[36m(pid=2518547)[0m INFO 02-20 20:52:54 __init__.py:183] Automatically detected platform cuda.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(run_inference_one_model pid=2518531)[0m WARNING 02-20 20:53:00 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:53:00 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_d9d27436'), local_subscribe_port=34595, remote_subscribe_port=None)
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:53:00 model_runner.py:1111] Starting to load model results/Qwen-2.5-1.5B-Simple-RL...
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:52:56 cuda.py:235] Using Flash Attention backend.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=2518538)[0m INFO 02-20 20:53:01 model_runner.py:1116] Loading model weights took 0.7655 GB
[36m(RayWorkerWrapper pid=2518538)[0m INFO 02-20 20:53:06 worker.py:266] Memory profiling takes 4.21 seconds
[36m(RayWorkerWrapper pid=2518538)[0m INFO 02-20 20:53:06 worker.py:266] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB
[36m(RayWorkerWrapper pid=2518538)[0m INFO 02-20 20:53:06 worker.py:266] model weights take 0.77GiB; non_torch_memory takes 0.33GiB; PyTorch activation peak memory takes 0.89GiB; the rest of the memory reserved for KV Cache is 33.46GiB.
[36m(RayWorkerWrapper pid=2518534)[0m INFO 02-20 20:53:00 utils.py:938] Found nccl from library libnccl.so.2[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=2518534)[0m INFO 02-20 20:53:00 pynccl.py:67] vLLM is using nccl==2.21.5[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=2518534)[0m WARNING 02-20 20:53:01 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.[32m [repeated 7x across cluster][0m
[36m(run_inference_one_model pid=2518532)[0m INFO 02-20 20:53:01 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_482225d5'), local_subscribe_port=58513, remote_subscribe_port=None)
[36m(RayWorkerWrapper pid=2518534)[0m INFO 02-20 20:53:01 model_runner.py:1111] Starting to load model results/Qwen-2.5-1.5B-Simple-RL...[32m [repeated 7x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:53:06 executor_base.py:108] # CUDA blocks: 153785, # CPU blocks: 18724
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:53:06 executor_base.py:113] Maximum concurrency for 32768 tokens per request: 75.09x
[36m(RayWorkerWrapper pid=2518534)[0m INFO 02-20 20:53:02 model_runner.py:1116] Loading model weights took 0.7655 GB[32m [repeated 7x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:53:11 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[36m(run_inference_one_model pid=2518531)[0m Capturing CUDA graph shapes:   3%|â–Ž         | 1/35 [00:00<00:27,  1.25it/s]
[36m(run_inference_one_model pid=2518531)[0m Capturing CUDA graph shapes:   6%|â–Œ         | 2/35 [00:01<00:27,  1.22it/s]
[36m(run_inference_one_model pid=2518532)[0m Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
[36m(run_inference_one_model pid=2518531)[0m Capturing CUDA graph shapes:  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:06<00:14,  1.74it/s][32m [repeated 16x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m Capturing CUDA graph shapes:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:11<00:09,  1.76it/s][32m [repeated 18x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m Capturing CUDA graph shapes:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:16<00:04,  1.72it/s][32m [repeated 18x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m Capturing CUDA graph shapes:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:18<00:01,  1.70it/s]
[36m(run_inference_one_model pid=2518531)[0m Capturing CUDA graph shapes:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:19<00:01,  1.69it/s]
[36m(run_inference_one_model pid=2518531)[0m Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:20<00:00,  1.62it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:20<00:00,  1.69it/s]
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:   0%|          | 0/99 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Capturing CUDA graph shapes:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:17<00:02,  1.72it/s][32m [repeated 8x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:   1%|          | 1/99 [00:02<03:43,  2.28s/it, est. speed input: 1243.37 toks/s, output: 14.01 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Capturing CUDA graph shapes:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:19<00:00,  1.74it/s][32m [repeated 4x across cluster][0m
[36m(run_inference_one_model pid=2518532)[0m Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:20<00:00,  1.67it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:20<00:00,  1.72it/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:   0%|          | 0/99 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  11%|â–ˆ         | 11/99 [00:07<00:52,  1.68it/s, est. speed input: 353.01 toks/s, output: 217.83 toks/s][32m [repeated 18x across cluster][0m
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/99 [00:13<00:40,  1.85it/s, est. speed input: 550.16 toks/s, output: 547.11 toks/s][32m [repeated 24x across cluster][0m
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/99 [00:18<00:15,  3.88it/s, est. speed input: 591.03 toks/s, output: 999.10 toks/s][32m [repeated 21x across cluster][0m
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 52/99 [00:24<00:41,  1.12it/s, est. speed input: 594.08 toks/s, output: 1253.62 toks/s][32m [repeated 18x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/99 [00:29<00:27,  1.42it/s, est. speed input: 612.06 toks/s, output: 1345.05 toks/s][32m [repeated 14x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/99 [00:35<00:37,  1.07s/it, est. speed input: 546.68 toks/s, output: 1352.83 toks/s][32m [repeated 9x across cluster][0m
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/99 [00:39<00:55,  1.91s/it, est. speed input: 492.22 toks/s, output: 1497.06 toks/s][32m [repeated 4x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 72/99 [00:45<00:32,  1.21s/it, est. speed input: 474.90 toks/s, output: 1499.61 toks/s][32m [repeated 9x across cluster][0m
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/99 [00:50<00:24,  1.27s/it, est. speed input: 439.24 toks/s, output: 1753.35 toks/s][32m [repeated 12x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/99 [13:09<50:41, 217.27s/it, est. speed input: 32.80 toks/s, output: 177.36 toks/s] [32m [repeated 6x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 87/99 [13:15<21:48, 109.06s/it, est. speed input: 33.61 toks/s, output: 257.31 toks/s][32m [repeated 2x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 91/99 [13:17<03:33, 26.75s/it, est. speed input: 34.99 toks/s, output: 419.35 toks/s]
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 92/99 [13:18<02:11, 18.80s/it, est. speed input: 35.30 toks/s, output: 459.97 toks/s]
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/99 [13:18<00:50, 10.18s/it, est. speed input: 35.93 toks/s, output: 541.29 toks/s]
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/99 [13:19<00:31,  7.92s/it, est. speed input: 36.15 toks/s, output: 581.31 toks/s]
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 96/99 [13:19<00:17,  5.91s/it, est. speed input: 36.39 toks/s, output: 621.89 toks/s]
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 97/99 [13:20<00:08,  4.42s/it, est. speed input: 36.59 toks/s, output: 662.27 toks/s]
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 98/99 [13:20<00:03,  3.33s/it, est. speed input: 36.77 toks/s, output: 702.56 toks/s]
[36m(run_inference_one_model pid=2518531)[0m Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/99 [13:17<05:43, 38.12s/it, est. speed input: 34.66 toks/s, output: 378.69 toks/s][32m [repeated 3x across cluster][0m
[36m(run_inference_one_model pid=2518531)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [13:20<00:00,  2.46s/it, est. speed input: 36.95 toks/s, output: 743.06 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [13:20<00:00,  8.09s/it, est. speed input: 36.95 toks/s, output: 743.06 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 82/99 [13:36<56:15, 198.57s/it, est. speed input: 28.38 toks/s, output: 153.49 toks/s] 
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/99 [13:38<38:49, 145.61s/it, est. speed input: 28.82 toks/s, output: 192.58 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/99 [13:42<26:32, 106.14s/it, est. speed input: 29.06 toks/s, output: 231.19 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 86/99 [13:42<12:50, 59.29s/it, est. speed input: 29.74 toks/s, output: 309.94 toks/s] 
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 87/99 [13:43<09:00, 45.08s/it, est. speed input: 30.07 toks/s, output: 349.37 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 88/99 [13:43<06:09, 33.60s/it, est. speed input: 30.40 toks/s, output: 388.77 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/99 [13:43<04:07, 24.72s/it, est. speed input: 30.69 toks/s, output: 428.06 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/99 [13:43<02:41, 17.96s/it, est. speed input: 30.98 toks/s, output: 467.40 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 91/99 [13:44<01:44, 13.02s/it, est. speed input: 31.24 toks/s, output: 506.58 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/99 [13:44<00:44,  7.33s/it, est. speed input: 31.73 toks/s, output: 585.13 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/99 [13:45<00:28,  5.60s/it, est. speed input: 31.97 toks/s, output: 624.48 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/99 [13:45<00:16,  4.22s/it, est. speed input: 32.18 toks/s, output: 663.78 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 96/99 [13:45<00:09,  3.19s/it, est. speed input: 32.38 toks/s, output: 702.96 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 98/99 [13:46<00:01,  1.92s/it, est. speed input: 32.75 toks/s, output: 781.41 toks/s]
[36m(run_inference_one_model pid=2518532)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [13:46<00:00,  1.54s/it, est. speed input: 32.91 toks/s, output: 820.59 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [13:46<00:00,  8.35s/it, est. speed input: 32.91 toks/s, output: 820.59 toks/s]
Splits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [14:59<00:00, 899.77s/it]Splits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [14:59<00:00, 899.77s/it]
[2025-02-20 21:07:31,808] [[32m    INFO[0m]: --- COMPUTING METRICS --- (pipeline.py:475)[0m
[2025-02-20 21:07:31,854] [[33m WARNING[0m]: We did not manage to extract a prediction in the correct format. Gold: ['B'], Pred: ['The given wave function is complex and not normalized. To find the value of "a," we need to calculate the integral of the square of the probability density function from x=1 to x=3 and set it equal to 1.\n\nFirst, let\'s find the square of the probability density function:\n\n|( a / sqrt(1 + x) ) - 0.5*i|^2 = a^2/(1+x) + 0.25\n\nNow, we integrate this with respect to x from x=1 to x=3:\n\n[Numerical integration gives the result 0.257]\n\nSetting this equal to 1:\n\n0.257a^2 = 1 solving gives a = âˆš4 = 2\n\nTherefore, none of the given options match the calculated value of "a". The correct value of "a" is 2.'] (dynamic_metrics.py:265)[0m
[2025-02-20 21:07:31,879] [[33m WARNING[0m]: We did not manage to extract a prediction in the correct format. Gold: ['C'], Pred: ['The total momentum of the system is the sum of the momenta of the two astronauts:\n\n{eq}\\displaystyle {p_{total}=3mc\\tanh\\beta_2+\\frac{2mc}{\\cosh\\beta_1}}\n\n{/eq}\n\nwhere {eq}\\displaystyle {\\beta_2 =0.5c/c=0.5}\n\n{/eq} and {eq}\\displaystyle {\\beta_1 =0.6c/c=0.6}\n\n{/eq}.\n\nSubstituting these values, we get:\n\n{eq}\\displaystyle {p_{total}=3mc\\tanh 0.5+(1.25)2mc = 4mc=mc_0}\n\n{/eq}\n\nwhere {eq}\\displaystyle {c_0=4c}\n\n{/eq} is the relative velocity of the system.\n\nThe relative kinetic energy is given by:\n\n{eq}\\displaystyle {E_k=\\frac{1}{2} total KE-p_{two pairs}\n\n}{/eq}\n\nwhere {eq}\\displaystyle {p_{two pairs}=0}\n\n{/eq} is the momentum after recombination.\n\nSubstituting the appropriate values, we get:\n\n{eq}\\displaystyle {E_k=\\frac{1}{2} (4mc^2)+(1.44)mc^2 =4.96 mc^2 }\n\n{/eq}\n\nSince the mass of the combination is (3m +2m) = (5m), giving option (a).\n####\nThe answer is **Total KE = 4.96 mc^2**\n**Relative velocity = 0.14c**'] (dynamic_metrics.py:265)[0m
[2025-02-20 21:07:32,065] [[32m    INFO[0m]: --- DISPLAYING RESULTS --- (pipeline.py:517)[0m
[2025-02-20 21:07:32,082] [[32m    INFO[0m]: --- SAVING AND PUSHING RESULTS --- (pipeline.py:507)[0m
[2025-02-20 21:07:32,082] [[32m    INFO[0m]: Saving experiment tracker (evaluation_tracker.py:180)[0m
[2025-02-20 21:07:35,090] [[32m    INFO[0m]: Saving results to /home/jiaminghui/Workspace/open-r1/eval_results/results/Qwen-2.5-1.5B-Simple-RL/results/results_Qwen-2.5-1.5B-Simple-RL/results_2025-02-20T21-07-32.083066.json (evaluation_tracker.py:234)[0m
[2025-02-20 21:07:35,091] [[32m    INFO[0m]: Saving details to /home/jiaminghui/Workspace/open-r1/eval_results/results/Qwen-2.5-1.5B-Simple-RL/details/results_Qwen-2.5-1.5B-Simple-RL/2025-02-20T21-07-32.083066 (evaluation_tracker.py:278)[0m

[36m(RayWorkerWrapper pid=2518534)[0m INFO 02-20 20:53:07 worker.py:266] Memory profiling takes 4.66 seconds[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=2518534)[0m INFO 02-20 20:53:07 worker.py:266] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=2518534)[0m INFO 02-20 20:53:07 worker.py:266] model weights take 0.77GiB; non_torch_memory takes 0.33GiB; PyTorch activation peak memory takes 0.89GiB; the rest of the memory reserved for KV Cache is 33.46GiB.[32m [repeated 7x across cluster][0m
[36m(run_inference_one_model pid=2518532)[0m INFO 02-20 20:53:07 executor_base.py:108] # CUDA blocks: 153785, # CPU blocks: 18724
[36m(run_inference_one_model pid=2518532)[0m INFO 02-20 20:53:07 executor_base.py:113] Maximum concurrency for 32768 tokens per request: 75.09x
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:53:32 model_runner.py:1563] Graph capturing finished in 21 secs, took 0.40 GiB
[36m(run_inference_one_model pid=2518531)[0m INFO 02-20 20:53:32 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 30.59 seconds
[36m(RayWorkerWrapper pid=2518541)[0m INFO 02-20 20:53:12 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=2518534)[0m INFO 02-20 20:53:32 model_runner.py:1563] Graph capturing finished in 20 secs, took 0.40 GiB[32m [repeated 7x across cluster][0m
[36m(run_inference_one_model pid=2518532)[0m INFO 02-20 20:53:32 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 30.68 seconds
|        Task         |Version|     Metric     |Value |   |Stderr|
|---------------------|------:|----------------|-----:|---|-----:|
|all                  |       |extractive_match|0.2525|Â±  | 0.031|
|custom:gpqa:diamond:0|      1|extractive_match|0.2525|Â±  | 0.031|

Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.11ba/s]
